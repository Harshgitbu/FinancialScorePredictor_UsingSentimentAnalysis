{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "%pip install beautifulsoup4 requests azure-storage-blob lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "microsoft": {}
      },
      "outputs": [],
      "source": [
        "conn_str = \"DefaultEndpointsProtocol=https;AccountName=ba870container;AccountKey= key;EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(conn_str)\n",
        "START_DATE = datetime(2021, 12, 27)\n",
        "END_DATE   = datetime(2024, 9, 5)\n",
        "container_name = \"bronze\"\n",
        "blob_name = \"yfinance_filtered_tickers.txt\"\n",
        "\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "content = blob_client.download_blob().readall().decode(\"utf-8\")\n",
        "\n",
        "# Parse tickers into a list\n",
        "tickers = [line.strip() for line in content.splitlines() if line.strip()]\n",
        "print(\"Loaded\", len(tickers), \"tickers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- MAP TICKERS TO COMPANY NAMES -----\n",
        "ticker_to_name = {}\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        company_name = yf.Ticker(ticker).info.get(\"shortName\", None)\n",
        "        if company_name:\n",
        "            ticker_to_name[ticker] = company_name\n",
        "            print(f\"{ticker} → {company_name}\")\n",
        "        time.sleep(0.5)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to get company name for {ticker}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_gnews_by_name(company_name, ticker, api_key):\n",
        "    base_url = \"https://gnews.io/api/v4/search\"\n",
        "    \n",
        "    params = {\n",
        "        \"q\": company_name,\n",
        "        \"lang\": \"en\",\n",
        "        \"from\": START_DATE.strftime(\"%Y-%m-%d\"),\n",
        "        \"to\": END_DATE.strftime(\"%Y-%m-%d\"),\n",
        "        \"token\": api_key,\n",
        "        \"max\": 100  # GNews free tier returns up to 100 articles per request\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    filtered_news = []\n",
        "\n",
        "    try:\n",
        "        results = response.json().get(\"articles\", [])\n",
        "\n",
        "        for item in results:\n",
        "            # Parse and filter again just to be safe\n",
        "            try:\n",
        "                published_str = item.get(\"publishedAt\")\n",
        "                if published_str:\n",
        "                    news_date = datetime.strptime(published_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                    if START_DATE <= news_date <= END_DATE:\n",
        "                        filtered_news.append({\n",
        "                            \"company\": company_name,\n",
        "                            \"ticker\": ticker,\n",
        "                            \"headline\": item.get(\"title\"),\n",
        "                            \"link\": item.get(\"url\"),\n",
        "                            \"timestamp\": news_date.isoformat(),\n",
        "                            \"source\": item.get(\"source\", {}).get(\"name\")\n",
        "                        })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error parsing GNews response for {company_name}: {e}\")\n",
        "\n",
        "    return filtered_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "GNEWS_API_KEY = \"08163c8807e33eb3b78e4c17a235b108\"\n",
        "for ticker, name in ticker_to_name.items():\n",
        "    try:\n",
        "        news_items = scrape_gnews_by_name(name, ticker, GNEWS_API_KEY)\n",
        "\n",
        "        if news_items:\n",
        "            now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            blob_name = f\"news/{ticker}/{ticker}_gnews_filtered_{now}.json\"\n",
        "\n",
        "            json_string = json.dumps(news_items)\n",
        "            output_blob_client = blob_service_client.get_blob_client(container=NEWS_OUTPUT_CONTAINER, blob=blob_name)\n",
        "            output_blob_client.upload_blob(json_string, overwrite=True)\n",
        "\n",
        "            print(f\"✅ Uploaded {len(news_items)} articles for {ticker} ({name})\")\n",
        "\n",
        "        else:\n",
        "            print(f\"⚠️ No articles in range for {ticker}:({name})\")\n",
        "\n",
        "        time.sleep(1.5)  # to avoid throttling by GNews\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to upload for {ticker}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
